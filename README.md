# micrograd

This is an introduction to how neural networks work, by Andrej Karpathy. Andrej Karpathy wrote his own library and implements backpropagation and the things needed for neural networks from scratch. It goes into how everything works, the math, the calculus including gradient descent.

## Manual Backpropagation with a Simple Expression
First, we review what derivatives are in python Jupiter notebook. Then, we create our own class, called Value. It can add and multiply and stores the children nodes which are the factors or summands of the expression. We used graphviz to draw the algebraic diagram of the expression, called an expression graph. It tells us the value and the operations applied to each value. Then we calculated the gradient of the final output, L, with respect to each variable. dL/da, dL/db, dL/de, etc. To get the gradient, we just take the partial derivative with respect to our variable. We used chain rule, so we just looked at the partial derivative of the intermediate expression, then multiplied it by the gradient of the output node. An example: dL/dc = dL/dd * dd/dc
We also double checked the gradients numerically and algebraically. Using the gradients, we can nudge the weights using a small fraction of the gradient.

## Manual Backpropagation with a Neuron
Next we want to use what we did and learned to represent an actual neuron in a neural network. It’s a matrix multiplication with inputs x1, x2, weights w1, w2, and bias b. Lastly we use an activation function or squashing operation to normalize the value, aka get the value into a specific range usually -1 to 1. We use the tanh activation function. We do the same thing we did before, calculating the gradients recursively from the output making our way left to the inputs. We start with the derivative of do/do = 1. The parent or first is always 1. Then the derivative of tanh(n) with respect to n, do/dn, which is 1 - tanh(n)^2. We do the rest just as we did before. For neural nets, we realize that there are just two types of partial derivatives. For addition, the gradient is 1 * the parent’s gradient, since the variable proportionally changes the output. So the gradient just trickles backward. The other is for multiplication, which is the other input’s data times the parent’s gradient. This is shown with a simple partial derivative, for example x1w1 = x1*w1; dx1w1/dx1 = w1, then w1 multiplied by the gradient of x1w1 is x1’s gradient. This can easily be coded for these two cases, addition and multiplication. Oh the third case is tanh.

## Implementing Backpropagation
Now that we’ve done backpropagation manually and understand how it works in depth, we want to code it so the computer can do it all when we run just run one function. Firstly, to do that, we implement the _backward function. Based on the three cases, +, *, and tanh, we compute the gradient accordingly. One crucial thing for me to understand how it worked was to understand the class Value and how each object is represented in the expression graph. The relationship of Value objects and the operations is that self and other (the kids) create the output, which is the parent. For the _backward function, I learned that we can store functions as variables in python. _backward is a variable that stores a function. So when there’s an operation, which is between two siblings, and we create a parent (the output), the parent which stores the operation then stores the function which calculates the gradient. That function, _backward(), which is unique based on the operation its kids used, calculates its kids’ gradients. Again, we call _backward() on a parent and it calculates the gradients for its kids. To do a semi-manual backprop with the _backward() function, we firstly have to manually set the gradient of the output, which is 1. Then we call _backward() and each node that has kids, starting from the output, right to left. We have to do this because we need to know the gradient of the parent. We only know the gradient of the output to begin with. We stop when we reach a leaf, i.e., a node that has no kids. We have out variable _prev for this, which is a tuple that stores the node’s children.

Now that we’ve got backprop done semi-manually, let’s get the class to do it all with just one function call. For this, all we need to do is call _backward on each Value object (or node) starting from the output then going left. The question is how do we choose the order of which we call the nodes? Remember our nodes are in a tree representation. The way that we order it is using topological sort, or topsort for short. It uses depth first search, but it adds the nodes to a list. That list contains all the nodes in our expression graph in the order that we call _backward on them. So yeah, just iterate over the list and call _backward on each item. Now, to do backprop, just call the backward() function on the head node, which is the final output node. One bug here is that backward() doesn’t work if you use a variable multiple times. This is because updating the gradient within function() will override the previous one. For partial derivatives, you want to add the new gradient to the existing one. All we do is instead of setting the gradients, we add to the gradients with a += sign. And now it works!
Next, we broke down the tanh operation into its equivalent form, tanh(x) = (e^(2x) - 1) / (e^(2x) + 1). And we represent that in the expression tree and run backward() on it. We expect to get the same gradients. This is the same equation, now just broken down more. So we need our Value class to support more operations, which is exponentiation, division, and subtraction. We add all the functions to the class, and I learned more about python, for example __rmul__, assert keyword, isinstance(), etc. We also add the _backward() function so that the class can get the gradient for each operation. The Value Class was really well thought out and how it works together is quite complex as a class. The operations build on each other. For example in Karpathy’s code, he uses the pow function to raise a value to the power -1 in order to divide. So with that, the divide function is just one line. He uses the same tricks for subtraction, which uses addition and negation, which is multiplying by negative to negate the second value. Genius.

So that’s MicroGrad working. Now we can translate what we learned from MicroGrad onto an actual framework used by professionals in production. We’ll use pytorch. We learn that it’s very similar to our Value class we made, except it’s called Tensors and it’s takes advantage of the fact that whole matrices are used as data, as opposed to the scalars we used in our Value class, to do things much more efficiently and quickly. We learn about how to use some of its basic functions.

## Creating a Neural Network with Micrograd
Now, let’s take MicroGrad to the next step. We will build a full neural network. We define classes for a Neuron, a Layer, and then a Multilayer Perceptron (MLP), which uses Value objects as its fundamental scalar data. Once we have that working, we now want to train our model to work and accurately predict data given its ground truths. We take random numbers as input and outputs (ground truths). This is our training data. Then we define the loss function, which tells us how we are doing, how off is our model’s prediction compared to the ground truth. We use the mean squared error loss, which is just the difference of the prediction and the ground truth, squared. The loss function is the sum of all the individual losses of each training example. So just like any function, we can draw the expression tree of the loss function, summed up over our 4 training examples. And it’s a huge graph with many many operations. And we can do backpropagation on this entire expression. We get the gradients of each node in this graph, and it’s very useful to us because we can nudge the weights of the nodes to change the loss value, which is the output. We want to decrease the loss value to increase our model’s accuracy.

## Training Our Neural Network with a Tiny Dataset
So in our class for the neural network, we then want to update our parameters. We create functions to gather all our parameters in a list. Now we can update our parameters. First we do a forward pass, making the prediction on our training data and putting the output into the loss function, the mean squared error. After forward pass, we do backward pass. Make sure to zero the gradients before backward pass or else the gradients will accumulate since we did += in our self-defined backward functions. Then we do backward pass. This calculates the gradients for all our values. Finally, we update. We choose a learning rate, alpha, and using the function we made to get all of the parameters, update each parameter, with the equation parameter = parameter + (-learning rate) * parameter’s gradient. To reduce the loss, if the gradient is negative, we want to increase the value of the weight. If the gradient is positive, we want to decrease the value of the weight. This is what the equation does. We iterate over those steps and we see the loss decrease. Then we put it into a nice loop. We see our model’s predictions do very well. It’s easy because the data size is small and the inputs are simple.
